{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/buyer/Library/Python/3.8/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/buyer/Library/Python/3.8/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import torch\n",
    "import logging\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from diffusers import (\n",
    "    StableDiffusionXLPipeline,\n",
    "    DPMSolverMultistepScheduler,\n",
    "    DDPMScheduler,\n",
    ")\n",
    "from diffusers.optimization import get_scheduler\n",
    "from diffusers.training_utils import EMAModel\n",
    "from transformers import CLIPTokenizer\n",
    "from huggingface_hub import notebook_login\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # Model settings\n",
    "    model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "    instance_prompt = \"photo of Eesha\"  # Replace 'xyz' with your identifier\n",
    "    class_prompt = \"photo of a person\"\n",
    "    \n",
    "    # Paths\n",
    "    output_dir = \"dreambooth-model\"\n",
    "    instance_data_dir = \"training-images\"\n",
    "    \n",
    "    # Training settings\n",
    "    num_training_steps = 1000\n",
    "    learning_rate = 1e-6\n",
    "    train_batch_size = 1\n",
    "    gradient_accumulation_steps = 1\n",
    "    image_size = 512\n",
    "    mixed_precision = \"fp16\"  # or \"no\" for full precision\n",
    "    \n",
    "    # Device configuration\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "config = Config()\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(config.output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersonalDataset(Dataset):\n",
    "    def __init__(self, instance_data_dir, instance_prompt, tokenizer, size=512):\n",
    "        self.instance_data_dir = Path(instance_data_dir)\n",
    "        self.instance_prompt = instance_prompt\n",
    "        self.tokenizer = tokenizer\n",
    "        self.size = size\n",
    "        \n",
    "        self.image_paths = [f for f in self.instance_data_dir.iterdir() \n",
    "                           if f.suffix.lower() in ['.jpg', '.jpeg', '.png', '.webp']]\n",
    "        \n",
    "        if len(self.image_paths) == 0:\n",
    "            raise ValueError(f\"No images found in {instance_data_dir}\")\n",
    "        \n",
    "        logger.info(f\"Found {len(self.image_paths)} images in {instance_data_dir}\")\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "            transforms.CenterCrop(size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5]),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            image = self.transform(image)\n",
    "            \n",
    "            example = {\n",
    "                \"input_ids\": self.tokenizer(\n",
    "                    self.instance_prompt,\n",
    "                    padding=\"max_length\",\n",
    "                    truncation=True,\n",
    "                    max_length=self.tokenizer.model_max_length,\n",
    "                    return_tensors=\"pt\",\n",
    "                ).input_ids[0],\n",
    "                \"images\": image,\n",
    "            }\n",
    "            return example\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading image {image_path}: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model():\n",
    "    # Login to Hugging Face\n",
    "    notebook_login()\n",
    "    \n",
    "    # Load the pipeline\n",
    "    pipeline = StableDiffusionXLPipeline.from_pretrained(\n",
    "        config.model_id,\n",
    "        torch_dtype=torch.float16 if config.mixed_precision == \"fp16\" else torch.float32,\n",
    "        use_safetensors=True,\n",
    "        variant=\"fp16\"\n",
    "    )\n",
    "    \n",
    "    # Enable memory efficient attention\n",
    "    pipeline.enable_xformers_memory_efficient_attention()\n",
    "    \n",
    "    # Move to device\n",
    "    pipeline = pipeline.to(config.device)\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "def prepare_dataset(pipeline):\n",
    "    dataset = PersonalDataset(\n",
    "        instance_data_dir=config.instance_data_dir,\n",
    "        instance_prompt=config.instance_prompt,\n",
    "        tokenizer=pipeline.tokenizer,\n",
    "        size=config.image_size\n",
    "    )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_function(pipeline, dataset):\n",
    "    # Prepare optimizer\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        pipeline.unet.parameters(),\n",
    "        lr=config.learning_rate,\n",
    "    )\n",
    "    \n",
    "    # Prepare scheduler\n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"constant\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=config.num_training_steps,\n",
    "    )\n",
    "    \n",
    "    # Progress bar\n",
    "    progress_bar = tqdm(range(config.num_training_steps))\n",
    "    progress_bar.set_description(\"Steps\")\n",
    "    global_step = 0\n",
    "    \n",
    "    # Training loop\n",
    "    for step in range(config.num_training_steps):\n",
    "        pipeline.train()\n",
    "        \n",
    "        # Get training sample\n",
    "        batch = dataset[step % len(dataset)]\n",
    "        \n",
    "        # Forward pass\n",
    "        loss = pipeline(\n",
    "            batch[\"input_ids\"].unsqueeze(0).to(config.device),\n",
    "            batch[\"images\"].unsqueeze(0).to(config.device),\n",
    "            return_dict=True\n",
    "        ).loss\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        if (step + 1) % config.gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.update(1)\n",
    "            global_step += 1\n",
    "            \n",
    "            # Log progress\n",
    "            if global_step % 10 == 0:\n",
    "                logger.info(f\"Step {global_step}: loss = {loss.detach().item():.4f}\")\n",
    "            \n",
    "            # Save checkpoint\n",
    "            if global_step % 100 == 0:\n",
    "                pipeline.save_pretrained(os.path.join(config.output_dir, f\"checkpoint-{global_step}\"))\n",
    "    \n",
    "    # Save final model\n",
    "    pipeline.save_pretrained(config.output_dir)\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images(pipeline, prompt, num_images=1):\n",
    "    \"\"\"Generate images using the fine-tuned model\"\"\"\n",
    "    images = pipeline(\n",
    "        prompt,\n",
    "        num_inference_steps=50,\n",
    "        guidance_scale=7.5,\n",
    "        num_images_per_prompt=num_images\n",
    "    ).images\n",
    "    \n",
    "    # Save images\n",
    "    os.makedirs(\"generated_images\", exist_ok=True)\n",
    "    for i, image in enumerate(images):\n",
    "        image.save(f\"generated_images/generated_{i}.png\")\n",
    "    \n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "159426a7f4104df89d71f6562c36139e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 19 files: 100%|██████████| 19/19 [03:08<00:00,  9.94s/it]\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:02<00:00,  3.04it/s]\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "Refer to https://github.com/facebookresearch/xformers for more information on how to install xformers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 20\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Setup model\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     pipeline \u001b[38;5;241m=\u001b[39m \u001b[43msetup_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Prepare dataset\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m prepare_dataset(pipeline)\n",
      "Cell \u001b[0;32mIn[6], line 14\u001b[0m, in \u001b[0;36msetup_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m StableDiffusionXLPipeline\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      7\u001b[0m     config\u001b[38;5;241m.\u001b[39mmodel_id,\n\u001b[1;32m      8\u001b[0m     torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16 \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mmixed_precision \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfp16\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfloat32,\n\u001b[1;32m      9\u001b[0m     use_safetensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     10\u001b[0m     variant\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfp16\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Enable memory efficient attention\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menable_xformers_memory_efficient_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Move to device\u001b[39;00m\n\u001b[1;32m     17\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mto(config\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/diffusers/pipelines/pipeline_utils.py:1635\u001b[0m, in \u001b[0;36mDiffusionPipeline.enable_xformers_memory_efficient_attention\u001b[0;34m(self, attention_op)\u001b[0m\n\u001b[1;32m   1602\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21menable_xformers_memory_efficient_attention\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_op: Optional[Callable] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1603\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1604\u001b[0m \u001b[38;5;124;03m    Enable memory efficient attention from [xFormers](https://facebookresearch.github.io/xformers/). When this\u001b[39;00m\n\u001b[1;32m   1605\u001b[0m \u001b[38;5;124;03m    option is enabled, you should observe lower GPU memory usage and a potential speed up during inference. Speed\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1633\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m   1634\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1635\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_use_memory_efficient_attention_xformers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_op\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/diffusers/pipelines/pipeline_utils.py:1661\u001b[0m, in \u001b[0;36mDiffusionPipeline.set_use_memory_efficient_attention_xformers\u001b[0;34m(self, valid, attention_op)\u001b[0m\n\u001b[1;32m   1658\u001b[0m modules \u001b[38;5;241m=\u001b[39m [m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m modules \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(m, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule)]\n\u001b[1;32m   1660\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m-> 1661\u001b[0m     \u001b[43mfn_recursive_set_mem_eff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/diffusers/pipelines/pipeline_utils.py:1651\u001b[0m, in \u001b[0;36mDiffusionPipeline.set_use_memory_efficient_attention_xformers.<locals>.fn_recursive_set_mem_eff\u001b[0;34m(module)\u001b[0m\n\u001b[1;32m   1649\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfn_recursive_set_mem_eff\u001b[39m(module: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m   1650\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_use_memory_efficient_attention_xformers\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1651\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_use_memory_efficient_attention_xformers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_op\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m   1654\u001b[0m         fn_recursive_set_mem_eff(child)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/diffusers/models/modeling_utils.py:273\u001b[0m, in \u001b[0;36mModelMixin.set_use_memory_efficient_attention_xformers\u001b[0;34m(self, valid, attention_op)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m--> 273\u001b[0m         \u001b[43mfn_recursive_set_mem_eff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/diffusers/models/modeling_utils.py:269\u001b[0m, in \u001b[0;36mModelMixin.set_use_memory_efficient_attention_xformers.<locals>.fn_recursive_set_mem_eff\u001b[0;34m(module)\u001b[0m\n\u001b[1;32m    266\u001b[0m     module\u001b[38;5;241m.\u001b[39mset_use_memory_efficient_attention_xformers(valid, attention_op)\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 269\u001b[0m     \u001b[43mfn_recursive_set_mem_eff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/diffusers/models/modeling_utils.py:269\u001b[0m, in \u001b[0;36mModelMixin.set_use_memory_efficient_attention_xformers.<locals>.fn_recursive_set_mem_eff\u001b[0;34m(module)\u001b[0m\n\u001b[1;32m    266\u001b[0m     module\u001b[38;5;241m.\u001b[39mset_use_memory_efficient_attention_xformers(valid, attention_op)\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 269\u001b[0m     \u001b[43mfn_recursive_set_mem_eff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/diffusers/models/modeling_utils.py:269\u001b[0m, in \u001b[0;36mModelMixin.set_use_memory_efficient_attention_xformers.<locals>.fn_recursive_set_mem_eff\u001b[0;34m(module)\u001b[0m\n\u001b[1;32m    266\u001b[0m     module\u001b[38;5;241m.\u001b[39mset_use_memory_efficient_attention_xformers(valid, attention_op)\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 269\u001b[0m     \u001b[43mfn_recursive_set_mem_eff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/diffusers/models/modeling_utils.py:266\u001b[0m, in \u001b[0;36mModelMixin.set_use_memory_efficient_attention_xformers.<locals>.fn_recursive_set_mem_eff\u001b[0;34m(module)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfn_recursive_set_mem_eff\u001b[39m(module: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_use_memory_efficient_attention_xformers\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 266\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_use_memory_efficient_attention_xformers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_op\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m    269\u001b[0m         fn_recursive_set_mem_eff(child)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/diffusers/models/attention_processor.py:387\u001b[0m, in \u001b[0;36mAttention.set_use_memory_efficient_attention_xformers\u001b[0;34m(self, use_memory_efficient_attention_xformers, attention_op)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    384\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMemory efficient attention is currently not supported for custom diffusion for attention processor type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    385\u001b[0m     )\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_xformers_available():\n\u001b[0;32m--> 387\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(\n\u001b[1;32m    388\u001b[0m         (\n\u001b[1;32m    389\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRefer to https://github.com/facebookresearch/xformers for more information on how to install\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    390\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m xformers\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    391\u001b[0m         ),\n\u001b[1;32m    392\u001b[0m         name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxformers\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    393\u001b[0m     )\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    396\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.cuda.is_available() should be True but is False. xformers\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m memory efficient attention is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    397\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m only available for GPU \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    398\u001b[0m     )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: Refer to https://github.com/facebookresearch/xformers for more information on how to install xformers"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Setup model\n",
    "    pipeline = setup_model()\n",
    "    \n",
    "    # Prepare dataset\n",
    "    dataset = prepare_dataset(pipeline)\n",
    "    \n",
    "    # Train model\n",
    "    logger.info(\"Starting training...\")\n",
    "    pipeline = training_function(pipeline, dataset)\n",
    "    \n",
    "    # Generate test image\n",
    "    logger.info(\"Generating test image...\")\n",
    "    test_prompt = f\"professional photo of {config.instance_prompt}, high quality, detailed face\"\n",
    "    generate_images(pipeline, test_prompt)\n",
    "    \n",
    "    logger.info(\"Training complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
